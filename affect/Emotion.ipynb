{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5Xan2tnR89K"
   },
   "source": [
    "# Emotions in context using the Emotic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook and code adapted from https://github.com/Tandon-A/emotic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# README\n",
    "\n",
    "Make sure the following dependencies are installed. We recommened using a new conda environment.\n",
    "\n",
    "- python=3.8\n",
    "- pytorch <= 1.8.1\n",
    "- scikit-learn\n",
    "- numpy\n",
    "- pandas\n",
    "- matplotlib\n",
    "\n",
    "## Links to EMOTIC dataset\n",
    "\n",
    "This project uses the <a href=\"http://sunai.uoc.edu/emotic/download.html\">EMOTIC dataset</a> and follows the methodology as introduced in the paper <a href=\"https://arxiv.org/pdf/2003.13401.pdf\">'Context based emotion recognition using EMOTIC dataset'</a>.\n",
    "\n",
    "To make it easier for you we have hosted the EMOTIC dataset as well, if you intend to make use of it.\n",
    "\n",
    "**Link to data**: https://tud365.sharepoint.com/:f:/s/ConversationalAgents2021/EoSyoFarxCJFoZrfkgft3iQBnpRRPZNavJ6La5ZfRhrFXw?e=61LcKf\n",
    "\n",
    "If you do not wish to train the model from scratch, section 12 will show you how you can use pretrained versions of the model along with a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please change the cuda version to whatever is supported by your GPU, in case you do not have have an nvidia GPU\n",
    "# run the commented command instead\n",
    "# !conda install pytorch=1.8.1 torchvision torchaudio cudatoolkit=11.1 -c pytorch-lts -c nvidia\n",
    "\n",
    "# !pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset pre-processing and conversion to .npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download the EMOTIC dataset and unzip it\n",
    "- Download the Annotations.zip and unzip it as well\n",
    "- Make sure you follow the below directory structure as this is important for later steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "emotic/data\n",
    "├── Annotations\n",
    "│   ├── Annotations.mat\n",
    "│   ├── demo.m\n",
    "│   ├── demo_single.m\n",
    "│   ├── displayAnnotation.m\n",
    "│   ├── displayAnnotation_multiple.m\n",
    "│   ├── README_demo.txt\n",
    "│   └── README_EMOTIC_annotationsStructure.pdf\n",
    "├── Annotations.zip\n",
    "├── emotic\n",
    "│   ├── ade20k\n",
    "│   ├── emodb_small\n",
    "│   ├── emotic_pre\n",
    "│   ├── framesdb\n",
    "│   └── mscoco\n",
    "└── emotic.zip\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing OpenCV "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to install OpenCV:\n",
    "1. If you do not have a GPU you can install it via conda\n",
    "    - conda install -c conda-forge  opencv\n",
    "2. In case you want a CUDA enabled version the path is a bit *treacherous*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to convert this into the npy format. This has the benefit of being faster to load and consume less memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python: can't open file 'emotic/mat2py.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# This will take a long time!!\n",
    "!python emotic/mat2py.py --data_dir emotic/data --generate_npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rbCWI0rkt8yp"
   },
   "source": [
    "<h1>Project context</h1>\n",
    "\n",
    "Humans use their facial features or expressions to convey how they feel, such as a person may smile when happy and scowl when angry. Historically, computer vision research has focussed on analyzing and learning these facial features to recognize emotions. \n",
    "However, these facial features are not universal and vary extensively across cultures and situations. \n",
    "\n",
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/Tandon-A/emotic/master/assets/face.jpg\"> <img src=\"https://raw.githubusercontent.com/Tandon-A/emotic/master/assets/full_scene.jpg\" width=\"400\">\n",
    "  <figcaption>Fig 1: a) (Facial feature) The person looks angry or in pain b) (Whole scene) The person looks elated.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "A scene context, as shown in the figure above, can provide additional information about the situations. This project explores the use of context in recognizing emotions in images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FhzX7KUihZqu"
   },
   "source": [
    "# Prepare places pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "colab_type": "code",
    "id": "uYgeeri3wdCM",
    "outputId": "e7600276-d296-4d16-dad0-7d7293210466"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Get Resnet18 model trained on places dataset. \n",
    "!mkdir ./places\n",
    "!wget http://places2.csail.mit.edu/models_places365/resnet18_places365.pth.tar -O ./places/resnet18_places365.pth.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "DyigpTRvws6W",
    "outputId": "8b98e435-75e7-40c5-c7ec-9922fd0d63c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting ./places\\resnet18_places365.pth.tar -> ./places\\resnet18_places365_py36.pth.tar\n"
     ]
    }
   ],
   "source": [
    "# Converting model weights to python3.6 format\n",
    "import torch\n",
    "from torch.autograd import Variable as V\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms as trn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "\n",
    "model_path = './places'\n",
    "archs = ['resnet18']\n",
    "for arch in archs:\n",
    "    model_file = os.path.join(model_path, '%s_places365.pth.tar' % arch)\n",
    "    save_file = os.path.join(model_path, '%s_places365_py36.pth.tar' % arch)\n",
    "\n",
    "    from functools import partial\n",
    "    import pickle\n",
    "    pickle.load = partial(pickle.load, encoding=\"latin1\")\n",
    "    pickle.Unpickler = partial(pickle.Unpickler, encoding=\"latin1\")\n",
    "    model = torch.load(model_file,\n",
    "                       map_location=lambda storage, loc: storage,\n",
    "                       pickle_module=pickle)\n",
    "    torch.save(model, save_file)\n",
    "    print('converting %s -> %s' % (model_file, save_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RhWL6Qi_w4qp",
    "outputId": "dd6271c6-54c4-4f56-ddd9-b83eb0f606ab"
   },
   "outputs": [],
   "source": [
    "# Saving the model weights to use ahead in the notebook\n",
    "import torch\n",
    "from torch.autograd import Variable as V\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from torchvision import transforms as trn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "\n",
    "# the architecture to use\n",
    "arch = 'resnet18'\n",
    "model_weight = os.path.join(model_path, 'resnet18_places365_py36.pth.tar')\n",
    "\n",
    "# create the network architecture\n",
    "model = models.__dict__[arch](num_classes=365)\n",
    "\n",
    "# model_weight = '%s_places365.pth.tar' % arch\n",
    "\n",
    "checkpoint = torch.load(model_weight, map_location=lambda storage, loc: storage) # model trained in GPU could be deployed in CPU machine like this!\n",
    "state_dict = {str.replace(k,'module.',''): v for k,v in checkpoint['state_dict'].items()} # the data parallel layer will add 'module' before each layer name\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "model.cpu()\n",
    "torch.save(model, os.path.join(model_path, 'res_context' + '.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ykNjfrUuhpbq"
   },
   "source": [
    "# General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\dansmachine\\miniconda3\\envs\\ca_affect_lab\\lib\\site-packages (1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dansmachine\\miniconda3\\envs\\ca_affect_lab\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\dansmachine\\miniconda3\\envs\\ca_affect_lab\\lib\\site-packages (from scikit-learn) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\dansmachine\\miniconda3\\envs\\ca_affect_lab\\lib\\site-packages (from scikit-learn) (1.20.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\dansmachine\\miniconda3\\envs\\ca_affect_lab\\lib\\site-packages (from scikit-learn) (0.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in c:\\users\\dansmachine\\miniconda3\\envs\\ca_affect_lab\\lib\\site-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vi-O8QgwvOQY",
    "outputId": "b08b8a76-2a53-4d3c-fd66-8b0a838c8054"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\DANSMA~1\\AppData\\Local\\Temp/ipykernel_20264/3663063364.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpyplot\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mPIL\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mImage\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mscipy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import scipy.io\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AD0pBBBYh2vW"
   },
   "source": [
    "# Emotic classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZfPKerg4TWkR"
   },
   "source": [
    "## Emotic Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has been adapted from the paper: <a href=\"https://arxiv.org/pdf/2003.13401.pdf\">'Context based emotion recognition using EMOTIC dataset'</a>\n",
    "\n",
    "**It is recommened that you read the paper to understand the rationale of the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/nw_arch.png)\n",
    "Image borrowed from linked paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZWt88EcJVu0c",
    "outputId": "45c7c38e-1491-47cf-d4c8-a1d88d0d59a6"
   },
   "outputs": [],
   "source": [
    "class Emotic(nn.Module):\n",
    "    ''' Emotic Model'''\n",
    "    def __init__(self, num_context_features, num_body_features):\n",
    "        super(Emotic, self).__init__()\n",
    "        self.num_context_features = num_context_features\n",
    "        self.num_body_features = num_body_features\n",
    "        self.fc1 = nn.Linear((self.num_context_features + num_body_features),\n",
    "                             256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.d1 = nn.Dropout(p=0.5)\n",
    "        self.fc_cat = nn.Linear(256, 26)\n",
    "        self.fc_cont = nn.Linear(256, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x_context, x_body):\n",
    "        context_features = x_context.view(-1, self.num_context_features)\n",
    "        body_features = x_body.view(-1, self.num_body_features)\n",
    "        fuse_features = torch.cat((context_features, body_features), 1)\n",
    "        fuse_out = self.fc1(fuse_features)\n",
    "        fuse_out = self.bn1(fuse_out)\n",
    "        fuse_out = self.relu(fuse_out)\n",
    "        fuse_out = self.d1(fuse_out)\n",
    "        cat_out = self.fc_cat(fuse_out)\n",
    "        cont_out = self.fc_cont(fuse_out)\n",
    "        return cat_out, cont_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zdzZGj6AxLaC"
   },
   "source": [
    "## Emotic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eKG5dNMXxlnm",
    "outputId": "f946e2dc-921d-4c8c-bfb9-b05129e90e0a"
   },
   "outputs": [],
   "source": [
    "class Emotic_PreDataset(Dataset):\n",
    "    ''' Custom Emotic dataset class. Use preprocessed data stored in npy files. '''\n",
    "    def __init__(self, x_context, x_body, y_cat, y_cont, transform,\n",
    "                 context_norm, body_norm):\n",
    "        super(Emotic_PreDataset, self).__init__()\n",
    "        self.x_context = x_context\n",
    "        self.x_body = x_body\n",
    "        self.y_cat = y_cat\n",
    "        self.y_cont = y_cont\n",
    "        self.transform = transform\n",
    "        self.context_norm = transforms.Normalize(\n",
    "            context_norm[0], context_norm[1]\n",
    "        )  # Normalizing the context image with context mean and context std\n",
    "        self.body_norm = transforms.Normalize(\n",
    "            body_norm[0], body_norm[1]\n",
    "        )  # Normalizing the body image with body mean and body std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_cat)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_context = self.x_context[index]\n",
    "        image_body = self.x_body[index]\n",
    "        cat_label = self.y_cat[index]\n",
    "        cont_label = self.y_cont[index]\n",
    "        return self.context_norm(\n",
    "            self.transform(image_context)), self.body_norm(\n",
    "                self.transform(image_body)), torch.tensor(\n",
    "                    cat_label, dtype=torch.float32), torch.tensor(\n",
    "                        cont_label, dtype=torch.float32) / 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JFuEQruAxQrK"
   },
   "source": [
    "## Emotic Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are custom loss functions. We need these are there is a direct relation between the continuous V-A-D values and the emotion labels therefore the loss needs to account for it as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ObffJVXkqsJg",
    "outputId": "19050d6b-6dbd-45c3-8c40-d54bb834c497"
   },
   "outputs": [],
   "source": [
    "class DiscreteLoss(nn.Module):\n",
    "    ''' Class to measure loss between categorical emotion predictions and labels.'''\n",
    "    def __init__(self, weight_type='mean', device=torch.device('cpu')):\n",
    "        super(DiscreteLoss, self).__init__()\n",
    "        self.weight_type = weight_type\n",
    "        self.device = device\n",
    "        if self.weight_type == 'mean':\n",
    "            self.weights = torch.ones((1, 26)) / 26.0\n",
    "            self.weights = self.weights.to(self.device)\n",
    "        elif self.weight_type == 'static':\n",
    "            self.weights = torch.FloatTensor([\n",
    "                0.1435, 0.1870, 0.1692, 0.1165, 0.1949, 0.1204, 0.1728, 0.1372,\n",
    "                0.1620, 0.1540, 0.1987, 0.1057, 0.1482, 0.1192, 0.1590, 0.1929,\n",
    "                0.1158, 0.1907, 0.1345, 0.1307, 0.1665, 0.1698, 0.1797, 0.1657,\n",
    "                0.1520, 0.1537\n",
    "            ]).unsqueeze(0)\n",
    "            self.weights = self.weights.to(self.device)\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        if self.weight_type == 'dynamic':\n",
    "            self.weights = self.prepare_dynamic_weights(target)\n",
    "            self.weights = self.weights.to(self.device)\n",
    "        loss = (((pred - target)**2) * self.weights)\n",
    "        return loss.sum()\n",
    "\n",
    "    def prepare_dynamic_weights(self, target):\n",
    "        target_stats = torch.sum(target, dim=0).float().unsqueeze(dim=0).cpu()\n",
    "        weights = torch.zeros((1, 26))\n",
    "        weights[target_stats != 0] = 1.0 / torch.log(\n",
    "            target_stats[target_stats != 0].data + 1.2)\n",
    "        weights[target_stats == 0] = 0.0001\n",
    "        return weights\n",
    "\n",
    "\n",
    "class ContinuousLoss_L2(nn.Module):\n",
    "    ''' Class to measure loss between continuous emotion dimension predictions and labels. Using l2 loss as base. '''\n",
    "    def __init__(self, margin=1):\n",
    "        super(ContinuousLoss_L2, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        labs = torch.abs(pred - target)\n",
    "        loss = labs**2\n",
    "        loss[(labs < self.margin)] = 0.0\n",
    "        return loss.sum()\n",
    "\n",
    "\n",
    "class ContinuousLoss_SL1(nn.Module):\n",
    "    ''' Class to measure loss between continuous emotion dimension predictions and labels. Using smooth l1 loss as base. '''\n",
    "    def __init__(self, margin=1):\n",
    "        super(ContinuousLoss_SL1, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        labs = torch.abs(pred - target)\n",
    "        loss = 0.5 * (labs**2)\n",
    "        loss[(labs > self.margin)] = labs[(labs > self.margin)] - 0.5\n",
    "        return loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-AMUYcy5h9cM"
   },
   "source": [
    "# Load preprocessed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below loads the pre-processed datasets. The preprocessing was done above when running the `mat2py.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "VSadne_Bc5va",
    "outputId": "20993809-6f85-4fa9-846b-c9fa90f8d7fe"
   },
   "outputs": [],
   "source": [
    "# Change data_src variable as per your drive\n",
    "data_src = 'emotic/data/emotic_pre/'\n",
    "\n",
    "# Load training preprocessed data\n",
    "train_context = np.load(os.path.join(data_src, 'train_context_arr.npy'))\n",
    "train_body = np.load(os.path.join(data_src, 'train_body_arr.npy'))\n",
    "train_cat = np.load(os.path.join(data_src, 'train_cat_arr.npy'))\n",
    "train_cont = np.load(os.path.join(data_src, 'train_cont_arr.npy'))\n",
    "\n",
    "# Load validation preprocessed data\n",
    "val_context = np.load(os.path.join(data_src, 'val_context_arr.npy'))\n",
    "val_body = np.load(os.path.join(data_src, 'val_body_arr.npy'))\n",
    "val_cat = np.load(os.path.join(data_src, 'val_cat_arr.npy'))\n",
    "val_cont = np.load(os.path.join(data_src, 'val_cont_arr.npy'))\n",
    "\n",
    "# Load testing preprocessed data\n",
    "test_context = np.load(os.path.join(data_src, 'test_context_arr.npy'))\n",
    "test_body = np.load(os.path.join(data_src, 'test_body_arr.npy'))\n",
    "test_cat = np.load(os.path.join(data_src, 'test_cat_arr.npy'))\n",
    "test_cont = np.load(os.path.join(data_src, 'test_cont_arr.npy'))\n",
    "\n",
    "# Categorical emotion classes\n",
    "cat = [\n",
    "    'Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion',\n",
    "    'Confidence', 'Disapproval', 'Disconnection', 'Disquietment',\n",
    "    'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem', 'Excitement',\n",
    "    'Fatigue', 'Fear', 'Happiness', 'Pain', 'Peace', 'Pleasure', 'Sadness',\n",
    "    'Sensitivity', 'Suffering', 'Surprise', 'Sympathy', 'Yearning'\n",
    "]\n",
    "\n",
    "cat2ind = {}\n",
    "ind2cat = {}\n",
    "for idx, emotion in enumerate(cat):\n",
    "    cat2ind[emotion] = idx\n",
    "    ind2cat[idx] = emotion\n",
    "\n",
    "print('train ', 'context ', train_context.shape, 'body', train_body.shape,\n",
    "      'cat ', train_cat.shape, 'cont', train_cont.shape)\n",
    "print('val ', 'context ', val_context.shape, 'body', val_body.shape, 'cat ',\n",
    "      val_cat.shape, 'cont', val_cont.shape)\n",
    "print('test ', 'context ', test_context.shape, 'body', test_body.shape, 'cat ',\n",
    "      test_cat.shape, 'cont', test_cont.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "JySFyUFZNgPy",
    "outputId": "7a8b4c42-ec0d-42a1-b45a-af6b27949b96"
   },
   "outputs": [],
   "source": [
    "# setting the batch size for training, increase if needed\n",
    "batch_size = 16\n",
    "\n",
    "# setting the mean and standard deviation for all channels(3) of the images \n",
    "# this is used for normalising the dataset\n",
    "context_mean = [0.4690646, 0.4407227, 0.40508908]\n",
    "context_std = [0.2514227, 0.24312855, 0.24266963]\n",
    "body_mean = [0.43832874, 0.3964344, 0.3706214]\n",
    "body_std = [0.24784276, 0.23621225, 0.2323653]\n",
    "context_norm = [context_mean, context_std]\n",
    "body_norm = [body_mean, body_std]\n",
    "\n",
    "# dataset transforms and augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "test_transform = transforms.Compose(\n",
    "    [transforms.ToPILImage(), transforms.ToTensor()])\n",
    "\n",
    "train_dataset = Emotic_PreDataset(train_context, train_body, train_cat,\n",
    "                                  train_cont, train_transform, context_norm,\n",
    "                                  body_norm)\n",
    "val_dataset = Emotic_PreDataset(val_context, val_body, val_cat, val_cont,\n",
    "                                test_transform, context_norm, body_norm)\n",
    "test_dataset = Emotic_PreDataset(test_context, test_body, test_cat, test_cont,\n",
    "                                 test_transform, context_norm, body_norm)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "print('train loader ', len(train_loader), 'val loader ', len(val_loader),\n",
    "      'test', len(test_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvPoFnAliZBC"
   },
   "source": [
    "# Prepare emotic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cMSaPqJyVyEW",
    "outputId": "6da4eb89-e1b8-4891-c8fe-aab416bebeb4"
   },
   "outputs": [],
   "source": [
    "model_path_places = './places'\n",
    "# NOTE:\n",
    "# since we need two models as shown in the diagram above\n",
    "# we make use of a ResNet-18 trained on the places dataset for the \"context\" network path\n",
    "# abd a standard torch pretrained ResNet-18 for the \"body\" path\n",
    "model_context = torch.load(os.path.join(model_path_places, 'res_context.pth'))\n",
    "model_body = models.resnet18(pretrained=True)\n",
    "\n",
    "emotic_model = Emotic(\n",
    "    list(model_context.children())[-1].in_features,\n",
    "    list(model_body.children())[-1].in_features)\n",
    "model_context = nn.Sequential(*(list(model_context.children())[:-1]))\n",
    "model_body = nn.Sequential(*(list(model_body.children())[:-1]))\n",
    "\n",
    "# print (summary(model_context, (3,224,224), device=\"cpu\"))\n",
    "# print (summary(model_body, (3,128,128), device=\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rE5qh_ljPOqs"
   },
   "source": [
    "## Prepare optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "I6-3FTclWAGh",
    "outputId": "134ffac3-76f7-4192-ed51-1a538e17673f"
   },
   "outputs": [],
   "source": [
    "for param in emotic_model.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in model_context.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model_body.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "opt = optim.Adam(\n",
    "    (list(emotic_model.parameters()) + list(model_context.parameters()) +\n",
    "     list(model_body.parameters())),\n",
    "    lr=0.001,\n",
    "    weight_decay=5e-4)\n",
    "scheduler = StepLR(opt, step_size=7, gamma=0.1)\n",
    "\n",
    "disc_loss = DiscreteLoss('dynamic', device)\n",
    "cont_loss_SL1 = ContinuousLoss_SL1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "hvUH2QxGjCEc"
   },
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "wqtB3MrzA3Uj",
    "outputId": "d1bbf51a-76ee-4d13-97d0-2bf22108d660"
   },
   "outputs": [],
   "source": [
    "# this function contains the engineering code for the trainer\n",
    "# contains logic to load a batch, forward pass, backward pass and validation step\n",
    "def train_emotic(epochs,\n",
    "                 model_path,\n",
    "                 opt,\n",
    "                 scheduler,\n",
    "                 models,\n",
    "                 disc_loss,\n",
    "                 cont_loss,\n",
    "                 cat_loss_param=0.5,\n",
    "                 cont_loss_param=0.5):\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    train_loss = list()\n",
    "    val_loss = list()\n",
    "\n",
    "    model_context, model_body, emotic_model = models\n",
    "    emotic_model.to(device)\n",
    "    model_context.to(device)\n",
    "    model_body.to(device)\n",
    "\n",
    "    for e in trange(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        emotic_model.train()\n",
    "        model_context.train()\n",
    "        model_body.train()\n",
    "\n",
    "        for images_context, images_body, labels_cat, labels_cont in tqdm(\n",
    "                train_loader, leave=False):\n",
    "            images_context = images_context.to(device)\n",
    "            images_body = images_body.to(device)\n",
    "            labels_cat = labels_cat.to(device)\n",
    "            labels_cont = labels_cont.to(device)\n",
    "\n",
    "            opt.zero_grad()\n",
    "\n",
    "            pred_context = model_context(images_context)\n",
    "            pred_body = model_body(images_body)\n",
    "\n",
    "            pred_cat, pred_cont = emotic_model(pred_context, pred_body)\n",
    "            cat_loss_batch = disc_loss(pred_cat, labels_cat)\n",
    "            cont_loss_batch = cont_loss(pred_cont * 10, labels_cont * 10)\n",
    "            loss = (cat_loss_param * cat_loss_batch) + (cont_loss_param *\n",
    "                                                        cont_loss_batch)\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        if e % 1 == 0:\n",
    "            print('epoch = %d training loss = %.4f' % (e, running_loss))\n",
    "        train_loss.append(running_loss)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        emotic_model.eval()\n",
    "        model_context.eval()\n",
    "        model_body.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images_context, images_body, labels_cat, labels_cont in tqdm(\n",
    "                    val_loader, leave=False):\n",
    "                images_context = images_context.to(device)\n",
    "                images_body = images_body.to(device)\n",
    "                labels_cat = labels_cat.to(device)\n",
    "                labels_cont = labels_cont.to(device)\n",
    "\n",
    "                pred_context = model_context(images_context)\n",
    "                pred_body = model_body(images_body)\n",
    "\n",
    "                pred_cat, pred_cont = emotic_model(pred_context, pred_body)\n",
    "                cat_loss_batch = disc_loss(pred_cat, labels_cat)\n",
    "                cont_loss_batch = cont_loss(pred_cont * 10, labels_cont * 10)\n",
    "                loss = (cat_loss_param * cat_loss_batch) + (cont_loss_param *\n",
    "                                                            cont_loss_batch)\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            if e % 1 == 0:\n",
    "                print('epoch = %d validation loss = %.4f' % (e, running_loss))\n",
    "        val_loss.append(running_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print('completed training')\n",
    "    emotic_model.to(\"cpu\")\n",
    "    model_context.to(\"cpu\")\n",
    "    model_body.to(\"cpu\")\n",
    "    torch.save(emotic_model, os.path.join(model_path, 'model_emotic.pth'))\n",
    "    torch.save(model_context, os.path.join(model_path, 'model_context.pth'))\n",
    "    torch.save(model_body, os.path.join(model_path, 'model_body.pth'))\n",
    "\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 6))\n",
    "    f.suptitle('emotic')\n",
    "    ax1.plot(range(0, len(train_loss)), train_loss, color='Blue')\n",
    "    ax2.plot(range(0, len(val_loss)), val_loss, color='Red')\n",
    "    ax1.legend(['train'])\n",
    "    ax2.legend(['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 991
    },
    "colab_type": "code",
    "collapsed": true,
    "hidden": true,
    "id": "i1KsKv_hwoUC",
    "outputId": "d2b1b6fd-fb90-46c6-815e-2d76e111c07e"
   },
   "outputs": [],
   "source": [
    "# be careful training can take a while\n",
    "train_emotic(15, './models', opt, scheduler, [model_context, model_body, emotic_model], disc_loss, cont_loss_SL1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "cDa4nuQvjGSa"
   },
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "AFCcFv4mnmRi",
    "outputId": "cf3ff83f-7176-422a-ebbe-a0d82d77607e"
   },
   "outputs": [],
   "source": [
    "def test_scikit_ap(cat_preds, cat_labels):\n",
    "    ap = np.zeros(26, dtype=np.float32)\n",
    "    for i in range(26):\n",
    "        ap[i] = average_precision_score(cat_labels[i, :], cat_preds[i, :])\n",
    "    print('ap', ap, ap.shape, ap.mean())\n",
    "    return ap.mean()\n",
    "\n",
    "\n",
    "def test_emotic_vad(cont_preds, cont_labels):\n",
    "    vad = np.zeros(3, dtype=np.float32)\n",
    "    for i in range(3):\n",
    "        vad[i] = np.mean(np.abs(cont_preds[i, :] - cont_labels[i, :]))\n",
    "    print('vad', vad, vad.shape, vad.mean())\n",
    "    return vad.mean()\n",
    "\n",
    "\n",
    "def get_thresholds(cat_preds, cat_labels):\n",
    "    thresholds = np.zeros(26, dtype=np.float32)\n",
    "    for i in range(26):\n",
    "        p, r, t = precision_recall_curve(cat_labels[i, :], cat_preds[i, :])\n",
    "        for k in range(len(p)):\n",
    "            if p[k] == r[k]:\n",
    "                thresholds[i] = t[k]\n",
    "                break\n",
    "    np.save('./thresholds.npy', thresholds)\n",
    "    return thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "KOeZRVdbUPNx",
    "outputId": "dfecf880-f7ed-4bb1-c355-f07ce839b899"
   },
   "outputs": [],
   "source": [
    "def test_data(models, device, data_loader, num_images):\n",
    "    model_context, model_body, emotic_model = models\n",
    "    cat_preds = np.zeros((num_images, 26))\n",
    "    cat_labels = np.zeros((num_images, 26))\n",
    "    cont_preds = np.zeros((num_images, 3))\n",
    "    cont_labels = np.zeros((num_images, 3))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_context.to(device)\n",
    "        model_body.to(device)\n",
    "        emotic_model.to(device)\n",
    "        model_context.eval()\n",
    "        model_body.eval()\n",
    "        emotic_model.eval()\n",
    "        indx = 0\n",
    "        print('starting testing')\n",
    "        for images_context, images_body, labels_cat, labels_cont in iter(\n",
    "                data_loader):\n",
    "            images_context = images_context.to(device)\n",
    "            images_body = images_body.to(device)\n",
    "\n",
    "            pred_context = model_context(images_context)\n",
    "            pred_body = model_body(images_body)\n",
    "            pred_cat, pred_cont = emotic_model(pred_context, pred_body)\n",
    "\n",
    "            cat_preds[indx:(\n",
    "                indx +\n",
    "                pred_cat.shape[0]), :] = pred_cat.to(\"cpu\").data.numpy()\n",
    "            cat_labels[indx:(\n",
    "                indx +\n",
    "                labels_cat.shape[0]), :] = labels_cat.to(\"cpu\").data.numpy()\n",
    "            cont_preds[indx:(\n",
    "                indx +\n",
    "                pred_cont.shape[0]), :] = pred_cont.to(\"cpu\").data.numpy() * 10\n",
    "            cont_labels[indx:(indx + labels_cont.shape[0]\n",
    "                              ), :] = labels_cont.to(\"cpu\").data.numpy() * 10\n",
    "            indx = indx + pred_cat.shape[0]\n",
    "\n",
    "    cat_preds = cat_preds.transpose()\n",
    "    cat_labels = cat_labels.transpose()\n",
    "    cont_preds = cont_preds.transpose()\n",
    "    cont_labels = cont_labels.transpose()\n",
    "    scipy.io.savemat('./cat_preds.mat', mdict={'cat_preds': cat_preds})\n",
    "    scipy.io.savemat('./cat_labels.mat', mdict={'cat_labels': cat_labels})\n",
    "    scipy.io.savemat('./cont_preds.mat', mdict={'cont_preds': cont_preds})\n",
    "    scipy.io.savemat('./cont_labels.mat', mdict={'cont_labels': cont_labels})\n",
    "    print('completed testing')\n",
    "    ap_mean = test_scikit_ap(cat_preds, cat_labels)\n",
    "    vad_mean = test_emotic_vad(cont_preds, cont_labels)\n",
    "    print(ap_mean, vad_mean)\n",
    "    return ap_mean, vad_mean\n",
    "\n",
    "\n",
    "print('completed cell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "hidden": true,
    "id": "oB69Xo-kLldG",
    "outputId": "83d7bc40-c0b9-4dd2-e4ae-5441034c220c"
   },
   "outputs": [],
   "source": [
    "val_ap, val_vad = test_data([model_context, model_body, emotic_model], device,\n",
    "                            val_loader, val_dataset.__len__())\n",
    "test_ap, test_vad = test_data([model_context, model_body, emotic_model],\n",
    "                              device, test_loader, test_dataset.__len__())\n",
    "\n",
    "print('validation Mean average precision=%.4f Mean VAD MAE=%.4f' %\n",
    "      (val_ap, val_vad))\n",
    "print('testing Mean average precision=%.4f Mean VAD MAE=%.4f' %\n",
    "      (test_ap, test_vad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "hidden": true,
    "id": "T-fc5LNp4len",
    "outputId": "a773fa6b-c31c-40c5-be1b-6c5e17291b55",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat_labels = scipy.io.loadmat('./cat_labels.mat')\n",
    "cat_preds = scipy.io.loadmat('./cat_preds.mat')\n",
    "cat_preds = cat_preds['cat_preds']\n",
    "cat_labels = cat_labels['cat_labels']\n",
    "print(cat_preds.shape, cat_labels.shape)\n",
    "\n",
    "#thesholds calculation for inference\n",
    "thresholds = get_thresholds(cat_preds, cat_labels)\n",
    "print(thresholds, thresholds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pretrained models\n",
    "\n",
    "In case you do not wish to train your own model, we have also made a pretrained model and a few helper functions available to help you make use of it with Furhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a demo image of Christian Bale (as Batman) with his suit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality when operating with images you aren't given bounding boxes to go with them. So, to get our own bounding boxes on the faces of people, we use YOLO_v3(You only look once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.yolo_inference import yolo_infer2\n",
    "from lib.emotic import Emotic\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('experiment/bale.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the categories\n",
    "# our model outputs numbers as categories, so we much have a mapping from the numbers to the emotions\n",
    "cat = ['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion', 'Confidence', 'Disapproval', 'Disconnection', \\\n",
    "          'Disquietment', 'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem', 'Excitement', 'Fatigue', 'Fear','Happiness', \\\n",
    "          'Pain', 'Peace', 'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise', 'Sympathy', 'Yearning']\n",
    "cat2ind = {}\n",
    "ind2cat = {}\n",
    "for idx, emotion in enumerate(cat):\n",
    "    cat2ind[emotion] = idx\n",
    "    ind2cat[idx] = emotion\n",
    "\n",
    "vad = ['Valence', 'Arousal', 'Dominance']\n",
    "ind2vad = {}\n",
    "for idx, continuous in enumerate(vad):\n",
    "    ind2vad[idx] = continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to normalise the demo image using the statistics of the training data the model was trained on\n",
    "context_mean = [0.4690646, 0.4407227, 0.40508908]\n",
    "context_std = [0.2514227, 0.24312855, 0.24266963]\n",
    "body_mean = [0.43832874, 0.3964344, 0.3706214]\n",
    "body_std = [0.24784276, 0.23621225, 0.2323653]\n",
    "context_norm = [context_mean, context_std]\n",
    "body_norm = [body_mean, body_std]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pay attention to the arguments to the function,\n",
    "# it takes a path to the image location, a result and model directory\n",
    "# the result directory also contains threshold information to make decisions(please do not delete)\n",
    "# write_op will write the output to a new file and return the location(2nd return variable)\n",
    "# return_op will return the processed image as the 3rd return variable\n",
    "\n",
    "xs, _, r = yolo_infer2('experiment/bale.jpeg', 'experiment/results/',\n",
    "            'experiment/models/', context_norm, body_norm, ind2cat, ind2vad, write_op=False, return_op=True)\n",
    "\n",
    "# the inference function returns a dict, top-level key is the bounding box. Each bounding box has sub keys:\n",
    "#                1. cont - for continuous V-A-D emotion values\n",
    "#                2. cat - for the top k categorical emotions\n",
    "print(xs)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like batman isn't impressed, neither is his suit, thanks YOLO. Yes this example was deliberately chosen to show a minor pitfall wherein YOLO_v3 recognises objects that may seem like faces. As a developer it is up to you how you choose to resolve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# in this example we are returning and writing the output image\n",
    "xs, saved_op_loc, r = yolo_infer2('experiment/marvel_blackwidow_yelena1.jpg', 'experiment/results/',\n",
    "            'experiment/models/', context_norm, body_norm, ind2cat, ind2vad, write_op=True, return_op=True)\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_op_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(saved_op_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evidenced above the function can give you a dictionary of V-A-D values for each detected face. Please feel free to incorporate this code in your project however you see fit. Below you'll find some pointers on how this could be integrated with Furhat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration with Furhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a part of the course project you are expected to make your own conversational bot, it should also make use of the emotional state of the user and respond accordingly. Below you'll find a few pointers for the same:\n",
    "\n",
    "## Furhat Camera Feed\n",
    "- One can refer to [Camera feed](https://docs.furhat.io/users/#camera-feed) docs, however you might encounter a few issues in getting it to work correctly (based on last year's experience)\n",
    "- If this doesn't work (highly likely), you are encouraged to make an inference server and grant it access to your camera\n",
    "\n",
    "## Alternatively\n",
    "- If you want to use more advanced models feel free to do so, you may even reuse the pretrained models provided with this notebook\n",
    "- You can use the inference function above and wrap it up with Flask (or other lib of your choice) and have it serve the inference API\n",
    "- You may need to make minor changes based on your design idea, we have listed a few pointers you may wish to address\n",
    "    - Perhaps the API needs to be equipped with extra code to capture an image?\n",
    "    - Are you letting Furhat access the camera? Then maybe you need to split the camera stream\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What we want from you\n",
    "\n",
    "\n",
    "- You are free to use code that is given with this lab. Emotion/affect recognition using the face is the bare minimum we expect to see in your implementation with Furhat - be creative with where you'd use this in the SpaceReceptionist (or a skill of your choosing)\n",
    "- What we have given you is a implementation that can be worked by most people, we encourage you to try and implement multimodal models that could potentially be more accurate\n",
    "- A successful incorporation with Furhat would require you to manage Furhat's behaviour with regards to the emotion detected\n",
    "- Ultimately there should be \n",
    "    1. a short demo ~30s-1min showing your bot reacting to changes in emotion\n",
    "    2. Prepare ground-truth labels and calculate the average accuracy of your system’s affect detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOST = '127.0.0.1'  # Standard loopback interface address (localhost)\n",
    "PORT = 65432        # Port to listen on (non-privileged ports are > 1023)\n",
    "\n",
    "with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "    s.bind((HOST, PORT))\n",
    "    s.listen(1)\n",
    "    conn, addr = s.accept()\n",
    "    with conn:\n",
    "        print('Connected by', addr)\n",
    "        while True:\n",
    "            data = conn.recv(1024)\n",
    "            if len(data) != 0:\n",
    "                print(data, len(data))\n",
    "                print('Pretent we run the inference model...')\n",
    "                conn.sendall(bytes('Happy\\n', 'utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calEmoModel(img):\n",
    "\n",
    "    # pay attention to the arguments to the function,\n",
    "    # it takes a path to the image location, a result and model directory\n",
    "    # the result directory also contains threshold information to make decisions(please do not delete)\n",
    "    # write_op will write the output to a new file and return the location(2nd return variable)\n",
    "    # return_op will return the processed image as the 3rd return variable\n",
    "\n",
    "    cv2.imwrite('image.jpg', img)\n",
    "\n",
    "    xs, _, r = yolo_infer2('experiment/bale.jpeg', 'experiment/results/',\n",
    "                           'experiment/models/', context_norm, body_norm, ind2cat, ind2vad, write_op=False, return_op=True)\n",
    "\n",
    "    # the inference function returns a dict, top-level key is the bounding box. Each bounding box has sub keys:\n",
    "    #                1. cont - for continuous V-A-D emotion values\n",
    "    #                2. cat - for the top k categorical emotions\n",
    "    print(xs)\n",
    "    cv2.imwrite('image.jpg', r)\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('./haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "class VideoCamera(object):\n",
    "    def __init__(self):\n",
    "        # capturing video\n",
    "        self.video = cv2.VideoCapture(0)\n",
    "\n",
    "    def __del__(self):\n",
    "        # releasing camera\n",
    "        self.video.release()\n",
    "\n",
    "    def get_frame(self):\n",
    "        # extracting frames\n",
    "        succes = False\n",
    "        ret, frame = self.video.read()\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        face_rects = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        max_x, max_y, max_w, max_h = 0, 0, 0, 0\n",
    "        for (x, y, w, h) in face_rects:\n",
    "            if w > max_w and h > max_h:\n",
    "                max_x, max_y, max_w, max_h = x, y, w, h\n",
    "        if max_w > 0 and max_h > 0:\n",
    "            frame = gray[max_y:max_y + max_h, max_x:max_x + max_w]\n",
    "            succes = True\n",
    "        return succes, frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from flask import Flask, Response, request\n",
    "import cv2\n",
    "import numpy as np\n",
    "from EmotionModel import calEmoModel\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "video_camera = VideoCamera()\n",
    "\n",
    "\n",
    "def gen(camera):\n",
    "    emotion_i = []\n",
    "    ret, img = camera.get_frame()\n",
    "    xs = calEmoModel(img)\n",
    "    # TODO: use image to obtain emotion, return emotion.\n",
    "    return 'Happy gen'\n",
    "\n",
    "\n",
    "@app.route('/emotion')\n",
    "def retEmotion():\n",
    "    print('This is running...')\n",
    "    print('Request from client: ', request.args['data'])\n",
    "    return Response(gen(video_camera))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    HOST = '0.0.0.0'\n",
    "    PORT = 5000\n",
    "    app.run(host=HOST, port=PORT, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOvEGajUxIXQTcOluZM1wCA",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "emotic.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "pycharm-db4f1ae1",
   "language": "python",
   "display_name": "PyCharm (affects-lab)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}